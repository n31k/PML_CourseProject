---
title: "Practical Machine Learning Course Project"
author: "n31k"
date: "Friday, February 20, 2015"
output: html_document
---


# Read In Data ------------------------------------------------------------

```{r readin}
library(caret)
pml_tr <- read.table('pml-training.csv', header=T, sep=',')
pml_ts <- read.table('pml-testing.csv', header=T, sep=',')
```



# Data Splitting ----------------------------------------------------------

I split the training data into training, test and validation subsets. 

```{r datasplit}
set.seed(33833)
inBuild <- createDataPartition(y=pml_tr$classe,
                               p=0.7, list = F)

validation <- pml_tr[-inBuild,]
buildData <- pml_tr[inBuild,]

inTrain <- createDataPartition(y=buildData$classe, 
                               p=0.7, list=F)

training <- buildData[inTrain, ]
testing <- buildData[-inTrain,]
```

# Exploratory Graphs ------------------------------------------------------




# Feature Election --------------------------------------------------------
I arbitrarily decided to use only the predictors from the accelerometer, the gyroscope
and the magnetometer.

```{r feats}
feat <- colnames(pml_tr)
feat2 <- feat[grep('^gyros|^acel|^magnet', feat)]

training2 <- training[c('classe', feat2)]
```
# Model Fitting -----------------------------------------------------------

I followed the methodology described in Week 4 Lecture 2: I combined two
approaches, random forests and boosting. These approaches are described in the
lecture as the most powerful ones. Also, combining them will make them even
more accurate. 

```{r fit1, eval=FALSE}
mod1 <- train(classe~., method='rf',data=training2)
mod2 <- train(classe~., method='gbm', data=training2)

pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)

predDF <- data.frame(pred1, pred2, classe=testing$classe)
combModFit <- train(classe~., method='rf',data=predDF)
combPred <- predict(combModFit, predDF)
```
In the following tables, one can find performance measures
for the individual fits, as well as the combined fit. Still, 
those measures for the combined fit are not very informative.
In the tables to come one may find the indices of performance
on the validation set. 

```{r fitassess}
with(predDF, {
  
  s1 <- confusionMatrix(pred1, classe)
  s2 <- confusionMatrix(pred2, classe)
  print(s1)
  print(s2)
  
  
  })

confusionMatrix(combPred, testing$classe)
```

# Predict on Validation Data Set ------------------------------------------

```{r predvalid}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combModFit, predVDF)
```
```{r fitassess2}
confusionMatrix(combPredV, validation$classe)
```

# Predict the 20 cases ----------------------------------------------------

Finally, we predict the 20 unknown cases. Submitting the predicted
values on the online form has shown that the predictions were all correct. 
```{r finalpreds}
pred1_ts <- predict(mod1, pml_ts[feat2])
pred2_ts <- predict(mod2, pml_ts[feat2])

pred2_tsDF <- data.frame(pred1=pred1_ts, pred2=pred2_ts)
combPred_ts <- predict(combModFit, pred2_tsDF)
```